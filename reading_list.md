# Reading List

## Survey and Report

|**Title**|**Conference/Institute**|**Year**|
|-----|:----------:|:----:|
|[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://dl.acm.org/doi/full/10.1145/3560815)|ACM Computing Surveys|Dec 2022|
|[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)|ArXiv|Dec 2022|
|[Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey](https://arxiv.org/abs/2302.09270)|ArXiv|Feb 2023|
|[A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226)| ArXiv | Mar 2023|
|[Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)| ArXiv| Fen 2023|
|[Emergent Abilities of Large Language Models](https://research.google/pubs/pub52065/)|TMLR|Jun 2022|
|[On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)|HAI|Jul 2021|


## Paper

|**Topic**|**Title**|**Conference**|**Year**|
|--------|-----|:----------:|:----:|
|Chatbots, toxic|[Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots](https://dl.acm.org/doi/abs/10.1145/3548606.3560599)|CCS|Nov 2022|
|ChatGPT, ethics|[Exploring AI Ethics of ChatGPT: A Diagnostic Analysis](https://arxiv.org/abs/2301.12867)|ArXiv|Jan 2023|
|ChatGPT, prompt|[A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382)|ArXiv|Feb 2023|
|ChatGPT, prompt|[Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness](https://arxiv.org/abs/2302.13793)|ArXiv|Feb 2023|
|ChatGPT, prompt|[More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models](https://arxiv.org/abs/2302.12173)|ArXiv|Feb 2023|
|Prompt|[Dynamic Prompting: A Unified Framework for Prompt Tuning](https://arxiv.org/abs/2303.02909)|ArXiv|Mar 2023|
|Adversarial prompt|[Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/pdf/2211.09527.pdf)|ArXiv|Nov 2022|
|Adversarial prompt|[Adversarial Prompting for Black Box Foundation](https://arxiv.org/pdf/2302.04237.pdf)|ArXiv|Feb 2023|
|ChatGPT|[Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/abs/2302.10198)|ArXiv|Mar 2023|
|LLM, prompt|[AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)|EMNLP|Nov 2020|
|LLM, prompt, toxic|[REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/pdf/2009.11462.pdf)|EMNLP|Sep 2020|
|LLM, ChatGPT|[Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)|ArXiv|Feb 2023|
|LLM|[The Science of Detecting LLM-Generated Texts](https://www.researchgate.net/publication/368684822_The_Science_of_Detecting_LLM-Generated_Texts)|-|Feb 2023|
|LLM|[SoK: On the Impossible Security of Very Large Foundation Models](https://arxiv.org/pdf/2209.15259.pdf)|ArXiv|Sep 2022|
|LLM|[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)|FAccT|Mar 2021|
|LLM, harm|[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)|ArXiv|Aug 2022|
|ChatGPT|[Linguistic ambiguity analysis in ChatGPT](https://arxiv.org/abs/2302.06426)|ArXiv|Feb 2023| 
|ChatGPT, robust|[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)|ArXic|Mar 2023|
|ChatGPT, robust|[Evaluating the Robustness of Discrete Prompts](https://arxiv.org/abs/2302.05619)|ArXiv|Feb 2023|
|GPT-3.5, robust|[How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks](https://arxiv.org/abs/2303.00293)|ArXiv|Mar 2023|
|ChatGPT, DALL-E|[A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning](https://arxiv.org/abs/2302.09068)|ArXiv|Feb 2023
|Code generation|[Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models](https://arxiv.org/pdf/2302.04012.pdf)|ArXiv|Feb 2023|
|Diffusion Models|[Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)|ArXiv|Feb 2023|
|LLM, Hardware|[Fixing Hardware Security Bugs with Large Language Models](https://arxiv.org/pdf/2302.01215.pdf)|ArXiv|Feb 2023|
|Hate, bot|[Hate Raids on Twitch: Echoes of the Past, New Modalities, and Implications for Platform Governance](https://kumarde.com/papers/hateraids.pdf)|CSCW|Jan 2023|
|Hate speech, adversarial attack|[TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/pdf/2203.09509.pdf)|ACL|Jul 2022|
|Hate speech, ChatGPT|[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/pdf/2302.07736.pdf)|ArXiv|Feb 2023|
|Sponge examples|[Sponge Examples: Energy-Latency Attacks on Neural Networks](https://ieeexplore.ieee.org/abstract/document/9581273)|EuroS&P|Sep 2021|
|Deepfake, LM|[Deepfake Text Detection: Limitations and Opportunities](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a019/1He7XJaERtC)|S&P|May 2023|

## Datasets
|**Title**|**Paper**|**Link**|
|-----|:----------:|:----:|
|Real Toxicity Prompts|[paper](https://www.semanticscholar.org/paper/RealToxicityPrompts%3A-Evaluating-Neural-Toxic-in-Gehman-Gururangan/399e7d8129c60818ee208f236c8dda17e876d21f)|[website](https://allenai.org/data/real-toxicity-prompts)
|Awesome ChatGPT Prompts|-|[github](https://github.com/f/awesome-chatgpt-prompts)|
|Anthropic's Red Team dataset|[paper](https://arxiv.org/abs/2209.07858)|[github](https://github.com/anthropics/hh-rlhf/tree/master/red-team-attempts)
|Wirting Prompts (Reddit: Prompts and motivation to create something out of nothing)|-|[reddit](https://www.reddit.com/r/WritingPrompts/)
|DiffusionDB (text-to-image)|-|[github](https://github.com/poloclub/diffusiondb)

## Article and Blog
- Introducing ChatGPT. [[link]](https://openai.com/blog/chatgpt)
- Aligning Language Models to Follow Instructions (InstructAPI). [[link]](https://openai.com/research/instruction-following)
- Reinforcement Learning from Human Feedback (RLHF). [[link]](https://openai.com/research/learning-from-human-preferences)
- Moderation API. [[link]](https://openai.com/blog/new-and-improved-content-moderation-tooling)
- How to format inputs to ChatGPT models. [[link]](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)
- Advanced Prompting. [[link]](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md)
- Adversarial Prompt Engineering. [[link]](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-adversarial.md)
- Prompt Injection Attacks against GPT-3. [[link]](https://simonwillison.net/2022/Sep/12/prompt-injection/)
- Exploring Prompt Injection Attacks. [[link]](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/)
- ChatGPTâ€™s creator made a free tool for detecting AI-generated text. [[link]](https://www.theverge.com/2023/1/31/23579942/chatgpt-ai-text-detection-openai-classifier)


