# Reading List

## Survey and Report

|**Title**|**Conference/Institute**|**Year**|
|-----|:----------:|:----:|
|[A Survey of Large language Models](https://arxiv.org/abs/2303.18223)|ArXiv|Mar 2023|
|[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://dl.acm.org/doi/full/10.1145/3560815)|ACM Computing Surveys|Dec 2022|
|[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)|ArXiv|Dec 2022|
|[Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey](https://arxiv.org/abs/2302.09270)|ArXiv|Feb 2023|
|[A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/abs/2303.04226)| ArXiv | Mar 2023|
|[Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)| ArXiv| Fen 2023|
|[Emergent Abilities of Large Language Models](https://research.google/pubs/pub52065/)|TMLR|Jun 2022|
|[On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)|HAI|Jul 2021|
|[A Categorical Archive of ChatGPT Failure](https://arxiv.org/pdf/2302.03494.pdf)|ArXiv|Feb 2023|


## Paper

### LLM and prompt engineering
|**Topic**|**Title**|**Conference**|**Year**|
|--------|-----|:----------:|:----:|
|LLM|[The Science of Detecting LLM-Generated Texts](https://www.researchgate.net/publication/368684822_The_Science_of_Detecting_LLM-Generated_Texts)|-|Feb 2023|
|LLM|[SoK: On the Impossible Security of Very Large Foundation Models](https://arxiv.org/pdf/2209.15259.pdf)|ArXiv|Sep 2022|
|LLM|[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922)|FAccT|Mar 2021|
|LLM, prompt|[AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)|EMNLP|Nov 2020|
|LLM, prompt, toxic|[REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/pdf/2009.11462.pdf)|EMNLP|Sep 2020|
|LLM, ChatGPT|[Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)|ArXiv|Feb 2023|
|ChatGPT|[Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/abs/2302.10198)|ArXiv|Mar 2023|
|ChatGPT|[Linguistic ambiguity analysis in ChatGPT](https://arxiv.org/abs/2302.06426)|ArXiv|Feb 2023| 
|ChatGPT, ethics|[Exploring AI Ethics of ChatGPT: A Diagnostic Analysis](https://arxiv.org/abs/2301.12867)|ArXiv|Jan 2023|
|ChatGPT, robust|[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)|ArXic|Mar 2023|
|ChatGPT, robust|[Evaluating the Robustness of Discrete Prompts](https://arxiv.org/abs/2302.05619)|ArXiv|Feb 2023|
|GPT-3.5, robust|[How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks](https://arxiv.org/abs/2303.00293)|ArXiv|Mar 2023|
|ChatGPT, prompt|[A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382)|ArXiv|Feb 2023|
|ChatGPT, prompt|[Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness](https://arxiv.org/abs/2302.13793)|ArXiv|Feb 2023|
|ChatGPT, prompt|[More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models](https://arxiv.org/abs/2302.12173)|ArXiv|Feb 2023|
|Prompt|[Dynamic Prompting: A Unified Framework for Prompt Tuning](https://arxiv.org/abs/2303.02909)|ArXiv|Mar 2023|
|Adversarial prompt|[Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/pdf/2211.09527.pdf)|ArXiv|Nov 2022|
|Adversarial prompt|[Adversarial Prompting for Black Box Foundation](https://arxiv.org/pdf/2302.04237.pdf)|ArXiv|Feb 2023|
|Generative AI, Misinformation| [Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions](https://jiaweizhou.me/assets/chi23_ai_misinfo.pdf)|CHI|Apr 2023|
|LLM, GPT-2, Memorization|[Extracting Training Data from Large Language Models](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf)|USENIX|Aug 2021|
|Generative AI, LLM|[On the Possibilities of AI-Generated Text Detection](https://arxiv.org/pdf/2304.04736.pdf)|ArXiv|Apr 2023|
|Robustness, Large Vision-Language Models|[On Evaluating Adversarial Robustness of Large Vision-Language Model](https://arxiv.org/pdf/2305.16934.pdf)|ArXiv|May 2023|

### Online hate-related
|**Topic**|**Title**|**Conference**|**Year**|
|--------|-----|:----------:|:----:|
|Hate, bot|[Hate Raids on Twitch: Echoes of the Past, New Modalities, and Implications for Platform Governance](https://kumarde.com/papers/hateraids.pdf)|CSCW|Jan 2023|
|Hate speech, adversarial attack|[TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/pdf/2203.09509.pdf)|ACL|Jul 2022|
|Hate speech, ChatGPT|[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/pdf/2302.07736.pdf)|ArXiv|Feb 2023|
|LLM, harm|[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)|ArXiv|Aug 2022|
|Dialogue System, online hate|[SAFETYKIT: First Aid for Measuring Safety in Open-domain Conversational Systems](https://iris.unibocconi.it/bitstream/11565/4053224/1/2022.acl-long.284.pdf)|ACL|May 2022|
|Dialogue System, online hate| [On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark](https://arxiv.org/pdf/2110.08466.pdf)|ACL|May 2022|
|Chatbots, toxic|[Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots](https://dl.acm.org/doi/abs/10.1145/3548606.3560599)|CCS|Nov 2022|
|Toxic, Persona, ChatGPT|[Toxicity in ChatGPT: Analyzing Persona-assigned Language Models](https://arxiv.org/pdf/2304.05335.pdf)|ArXiv|Apr 2023|
|Hateful, offensive, toxic, ChatGPT|["HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media](https://arxiv.org/pdf/2304.10619.pdf)|ArXiv|Apr 2023|
|Hate Speech, Detection|[A Holistic Approach to Undesired Content Detection in the Real World](efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2208.03274.pdf)|AAAI|Feb 2023|
|NSFW, Text-to-Image, DALLE|[SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models’ Safety Filters](https://arxiv.org/pdf/2305.12082.pdf)|ArXiv|May 2023|



### Others
|**Topic**|**Title**|**Conference**|**Year**|
|--------|-----|:----------:|:----:|
|Sponge Attack|[Sponge Examples: Energy-Latency Attacks on Neural Networks](https://ieeexplore.ieee.org/abstract/document/9581273)|EuroS&P|Sep 2021|
|Sponge Attack|[Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors](https://ieeexplore.ieee.org/abstract/document/10030526)|WACV|Mar 2023|
|Deepfake, LM|[Deepfake Text Detection: Limitations and Opportunities](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a019/1He7XJaERtC)|S&P|May 2023|
|ChatGPT, DALL-E|[A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning](https://arxiv.org/abs/2302.09068)|ArXiv|Feb 2023
|Code generation|[Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models](https://arxiv.org/pdf/2302.04012.pdf)|ArXiv|Feb 2023|
|Diffusion Models|[Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)|ArXiv|Feb 2023|
|LLM, Hardware|[Fixing Hardware Security Bugs with Large Language Models](https://arxiv.org/pdf/2302.01215.pdf)|ArXiv|Feb 2023|
|SmartHome, Human-centered computing|[Demystifying the Vetting Process of Voice-controlled Skills on Markets](https://dl.acm.org/doi/pdf/10.1145/3478101)|IMWUT|Sep 2021|
|VPA|[SkillExplorer: Understanding the Behavior of Skills in Large Scale](https://www.usenix.org/system/files/sec20-guo.pdf)|USENIX| Aug 2020|
|Adversarial example, Seq-to-Seq models|[Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples](https://arxiv.org/abs/1803.01128)|AAAI|Apr 2020|
|Adversarial example, NLP|[Universal Adversarial Triggers for Attacking and Analyzing NLP](https://arxiv.org/abs/1908.07125)|EMNLP| Jan 2021|
|Aversarial example, NLP|[HotFlip: White-Box Adversarial Examples for Text Classification](https://arxiv.org/abs/1712.06751)|ACL|July 2018|
Bug Fixing, ChatGPT| [Keep the Conversation Going:Fixing 162 out of 337 bugs for $0.42 each using ChatGPT](https://arxiv.org/abs/2304.00385)| ArXiv|Apr 2023|

## Datasets
### Prompt dataset
|**Title**|**Paper**|**Link**|
|-----|:----------:|:----:|
|Real Toxicity Prompts|[paper](https://www.semanticscholar.org/paper/RealToxicityPrompts%3A-Evaluating-Neural-Toxic-in-Gehman-Gururangan/399e7d8129c60818ee208f236c8dda17e876d21f)|[website](https://allenai.org/data/real-toxicity-prompts)
|Awesome ChatGPT Prompts|-|[github](https://github.com/f/awesome-chatgpt-prompts)|
|Anthropic's Red Team dataset|[paper](https://arxiv.org/abs/2209.07858)|[github](https://github.com/anthropics/hh-rlhf/tree/master/red-team-attempts)
|Wirting Prompts (Reddit: Prompts and motivation to create something out of nothing)|-|[reddit](https://www.reddit.com/r/WritingPrompts/)
|DiffusionDB (text-to-image)|-|[github](https://github.com/poloclub/diffusiondb)|

### Hate speech dataset
|**Title**|**Paper**|**Link**|**Year**|
|-----|:----------:|:----:|:----:|
|Hate Speech Dataset List|-|[github](https://github.com/leondz/hatespeechdata)| - |
|Racism is a Virus: Anti-Asian Hate and Counterspeech in Social Media during the COVID-19 Crisis| [paper](https://arxiv.org/pdf/2005.12423.pdf) ASONAM|[dropbox](https://www.dropbox.com/sh/g9uglvl3cd61k69/AACEk2O2BEKwRTcGthgROOcWa?dl=0)| Nov 2021 |
|Call me sexist, but...”: Revisiting Sexism Detection Using Psychological Scalesand Adversarial Samples|[paper](https://ojs.aaai.org/index.php/ICWSM/article/view/18085/17888) ICWSM| [link](https://doi.org/10.7802/2251)| Jun 2021 |
|Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection|[paper](https://aclanthology.org/2021.wassa-1.18/) WASSA | [link](https://www.ims.uni-stuttgart.de/data/stance_hof_us2020)| Apr 2021|
|Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate|[paper](https://arxiv.org/abs/2108.05921) NAACL|[github](https://github.com/HannahKirk/Hatemoji) |Aug 2021|
|HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection|[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17745)<br>AAAI|[github](https://github.com/hate-alert/HateXplain)|Dec 2020|

## Journal
|**Topic**|**Title**|**Journal**|**Year**|
|-----|:----------:|:----:|:----:|
|Hate Speech|[A Literature Review of Textual Hate Speech Detection Methods and Datasets](https://www.mdpi.com/2078-2489/13/6/273)|Information|Apr 2022|

## Article and Blog
- Introducing ChatGPT. [[link]](https://openai.com/blog/chatgpt)
- Aligning Language Models to Follow Instructions (InstructAPI). [[link]](https://openai.com/research/instruction-following)
- Reinforcement Learning from Human Feedback (RLHF). [[link]](https://openai.com/research/learning-from-human-preferences)
- Moderation API. [[link]](https://openai.com/blog/new-and-improved-content-moderation-tooling)
- GPT-4 system card. [[link]](https://cdn.openai.com/papers/gpt-4-system-card.pdf)
- How Do OpenAI’s Efforts To Make GPT-4 “Safer” Stack Up Against The NIST AI Risk Management Framework? [[link]](https://fas.org/publication/how-do-openais-efforts-to-make-gpt-4-safer-stack-up-against-the-nist-ai-risk-management-framework/)
- How to format inputs to ChatGPT models. [[link]](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)
- Advanced Prompting. [[link]](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md)
- Adversarial Prompt Engineering. [[link]](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-adversarial.md)
- Prompt Injection Attacks against GPT-3. [[link]](https://simonwillison.net/2022/Sep/12/prompt-injection/)
- Exploring Prompt Injection Attacks. [[link]](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/)
- ChatGPT’s creator made a free tool for detecting AI-generated text. [[link]](https://www.theverge.com/2023/1/31/23579942/chatgpt-ai-text-detection-openai-classifier)
- Chatbot Safety with Prompt Evaluator for Chat-GPT [[link]](https://www.aligned-ai.com/post/chatbot-safety-with-prompt-evaluator-for-chat-gpt)
- Stanford Alpaca [[GitHub]](https://github.com/tatsu-lab/stanford_alpaca)
- Large Language Models like ChatGPT say The Darnedest Things [[link]](https://garymarcus.substack.com/p/large-language-models-like-chatgpt)
- OpenAI: Our approach to AI safety [[link]](https://openai.com/blog/our-approach-to-ai-safety)
- Samsung bans use of generative AI tools like ChatGPT after April internal data leak [[link]](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAJ0WKi4Rrq9ggXtDS1bBNVxjCz8-fsPEN4DIBlKD6sf4SRdoKdYw3gu-KnCj9mhZY27Mb7g8QBzATEAZSKI70WlYKviArHcsqs94qdxTLUKvpFG6C3ir_n5tiWxwq49Gmk9AvEUOtmcSy6fUekYTaKlVMnIi2gnWcS09Yz6TcJHc)
- FACT SHEET: Biden-⁠Harris Administration Announces New Actions to Promote Responsible AI Innovation that Protects Americans’ Rights and Safety [[link]](https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/)

- FACT SHEET: Biden-Harris Administration Takes New Steps to Advance Responsible Artificial Intelligence Research, Development, and Deployment | The White House[[link]](https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/23/fact-sheet-biden-harris-administration-takes-new-steps-to-advance-responsible-artificial-intelligence-research-development-and-deployment/)
